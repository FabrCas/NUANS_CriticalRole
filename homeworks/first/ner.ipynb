{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EPylhgmK8Y7i"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uP1lUJGj7F9S",
        "outputId": "147d7459-5a2b-441a-96f0-e8ad7af6a8d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: editdistance==0.3.1 in /usr/local/lib/python3.7/dist-packages (0.3.1)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_trf')\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: spacy-transformers in /usr/local/lib/python3.7/dist-packages (1.1.8)\n",
            "Requirement already satisfied: transformers<4.22.0,>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy-transformers) (4.21.3)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from spacy-transformers) (1.12.1+cu113)\n",
            "Requirement already satisfied: spacy-alignments<1.0.0,>=0.7.2 in /usr/local/lib/python3.7/dist-packages (from spacy-transformers) (0.8.6)\n",
            "Requirement already satisfied: spacy<4.0.0,>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy-transformers) (3.4.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy-transformers) (2.4.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.4.0->spacy-transformers) (1.0.9)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.4.0->spacy-transformers) (3.0.10)\n",
            "Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.4.0->spacy-transformers) (4.1.1)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.4.0->spacy-transformers) (8.1.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.4.0->spacy-transformers) (0.10.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.4.0->spacy-transformers) (3.3.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.4.0->spacy-transformers) (2.11.3)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.4.0->spacy-transformers) (2.0.7)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.4.0->spacy-transformers) (1.0.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.4.0->spacy-transformers) (21.3)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.4.0->spacy-transformers) (1.21.6)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.4.0->spacy-transformers) (2.23.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.4.0->spacy-transformers) (1.10.2)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.4.0->spacy-transformers) (2.0.8)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.4.0->spacy-transformers) (0.4.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.4.0->spacy-transformers) (4.64.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.4.0->spacy-transformers) (57.4.0)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.4.0->spacy-transformers) (0.6.2)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.4.0->spacy-transformers) (3.0.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<4.0.0,>=3.4.0->spacy-transformers) (3.10.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<4.0.0,>=3.4.0->spacy-transformers) (3.0.9)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<4.0.0,>=3.4.0->spacy-transformers) (5.2.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.4.0->spacy-transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.4.0->spacy-transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.4.0->spacy-transformers) (2022.9.24)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.4.0->spacy-transformers) (3.0.4)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<4.0.0,>=3.4.0->spacy-transformers) (0.0.3)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<4.0.0,>=3.4.0->spacy-transformers) (0.7.9)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers<4.22.0,>=3.4.0->spacy-transformers) (0.12.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers<4.22.0,>=3.4.0->spacy-transformers) (0.11.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers<4.22.0,>=3.4.0->spacy-transformers) (4.13.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<4.22.0,>=3.4.0->spacy-transformers) (3.8.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<4.22.0,>=3.4.0->spacy-transformers) (2022.6.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers<4.22.0,>=3.4.0->spacy-transformers) (6.0)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy<4.0.0,>=3.4.0->spacy-transformers) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<4.0.0,>=3.4.0->spacy-transformers) (2.0.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: spacy[cuda112] in /usr/local/lib/python3.7/dist-packages (3.4.2)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy[cuda112]) (1.21.6)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy[cuda112]) (2.0.8)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy[cuda112]) (0.4.2)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy[cuda112]) (1.10.2)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy[cuda112]) (3.0.8)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy[cuda112]) (1.0.9)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy[cuda112]) (4.64.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy[cuda112]) (21.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy[cuda112]) (57.4.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy[cuda112]) (2.0.7)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy[cuda112]) (3.3.0)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy[cuda112]) (2.4.5)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy[cuda112]) (1.0.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy[cuda112]) (2.23.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy[cuda112]) (2.11.3)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.7/dist-packages (from spacy[cuda112]) (3.0.10)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy[cuda112]) (8.1.5)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy[cuda112]) (0.6.2)\n",
            "Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy[cuda112]) (4.1.1)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy[cuda112]) (0.10.1)\n",
            "Requirement already satisfied: cupy-cuda112<12.0.0,>=5.0.0b4 in /usr/local/lib/python3.7/dist-packages (from spacy[cuda112]) (10.6.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy[cuda112]) (3.10.0)\n",
            "Requirement already satisfied: fastrlock>=0.5 in /usr/local/lib/python3.7/dist-packages (from cupy-cuda112<12.0.0,>=5.0.0b4->spacy[cuda112]) (0.8.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy[cuda112]) (3.0.9)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy[cuda112]) (5.2.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy[cuda112]) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy[cuda112]) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy[cuda112]) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy[cuda112]) (2022.9.24)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy[cuda112]) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy[cuda112]) (0.0.3)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy[cuda112]) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy[cuda112]) (2.0.1)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_md')\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: spacy-entity-linker in /usr/local/lib/python3.7/dist-packages (1.0.2)\n",
            "Requirement already satisfied: spacy>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy-entity-linker) (3.4.2)\n",
            "Requirement already satisfied: numpy>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy-entity-linker) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.0->spacy-entity-linker) (4.1.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.0->spacy-entity-linker) (57.4.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.0->spacy-entity-linker) (3.3.0)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.0->spacy-entity-linker) (0.6.2)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.0->spacy-entity-linker) (2.0.8)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.0->spacy-entity-linker) (2.23.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.0->spacy-entity-linker) (1.0.9)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.0->spacy-entity-linker) (0.4.2)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.0->spacy-entity-linker) (8.1.5)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.0->spacy-entity-linker) (3.0.8)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.0->spacy-entity-linker) (2.4.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.0->spacy-entity-linker) (0.10.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.0->spacy-entity-linker) (1.10.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.0->spacy-entity-linker) (2.11.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.0->spacy-entity-linker) (4.64.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.0->spacy-entity-linker) (2.0.7)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.0->spacy-entity-linker) (1.0.3)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.0->spacy-entity-linker) (3.0.10)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.0->spacy-entity-linker) (21.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy>=3.0.0->spacy-entity-linker) (3.10.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy>=3.0.0->spacy-entity-linker) (3.0.9)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy>=3.0.0->spacy-entity-linker) (5.2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.0.0->spacy-entity-linker) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.0.0->spacy-entity-linker) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.0.0->spacy-entity-linker) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.0.0->spacy-entity-linker) (3.0.4)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy>=3.0.0->spacy-entity-linker) (0.0.3)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy>=3.0.0->spacy-entity-linker) (0.7.9)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy>=3.0.0->spacy-entity-linker) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy>=3.0.0->spacy-entity-linker) (2.0.1)\n",
            "2022-11-20 17:17:44.965363: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n"
          ]
        }
      ],
      "source": [
        "# - imports parameters \n",
        "PIPELINES_TYPE = [\"en_core_web_sm\",\"en_core_web_md\",\\\n",
        "                 \"en_core_web_lg\",\"en_core_web_trf\"]\n",
        "SIZE_IDX = 3 # use transformer models\n",
        "EL_RESOURCE = \"wikidata\" # what kind of resource use for Entity linking\n",
        "\n",
        "# - general imports \n",
        "from google.colab       import files\n",
        "from google.colab       import drive\n",
        "import os\n",
        "import sys\n",
        "import numpy                            as np\n",
        "import re\n",
        "import random\n",
        "import copy\n",
        "import time\n",
        "import pandas                           as pd\n",
        "from zipfile            import ZipFile\n",
        "import requests\n",
        "import json\n",
        "import nltk\n",
        "from nltk.corpus        import stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# fast implementation of edit distance\n",
        "!pip install editdistance==0.3.1\n",
        "import editdistance\n",
        "\n",
        "# - spacy imports for NER\n",
        "from spacy.cli.download import download as spacy_download\n",
        "spacy_download(PIPELINES_TYPE[SIZE_IDX])\n",
        "if SIZE_IDX==3:\n",
        "  !pip install spacy-transformers\n",
        "  import spacy_transformers\n",
        "!pip install -U spacy[cuda112]\n",
        "import spacy\n",
        "spacy.prefer_gpu()\n",
        "\n",
        "# - Entity linking imports \n",
        "if EL_RESOURCE == \"wikidata\":\n",
        "  spacy_download(PIPELINES_TYPE[1])\n",
        "  !pip install spacy-entity-linker\n",
        "  !python -m spacy_entity_linker \"download_knowledge_base\"\n",
        "else:\n",
        "  import urllib.request as urllib2\n",
        "  import urllib\n",
        "  import json\n",
        "  import gzip\n",
        "  import io \n",
        "\n",
        "\n",
        "# IMPORTANT: one restart of runtime it's needed to correctly install all the packages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BHEAwNPu8pKq"
      },
      "source": [
        "#Load the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "UaKkDV6R320x",
        "outputId": "be37905a-7b6f-4076-c7a4-2ed397cbcdf6"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-449233a2-c394-4242-86d6-6cda9080257d\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-449233a2-c394-4242-86d6-6cda9080257d\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving FairySum.zip to FairySum.zip\n",
            "/content/FairySum\n",
            "-Loaded: ./FAIRY_TALE/texts/bn_00173084n_The Old Dame and her Hen.txt\n",
            "-Loaded: ./FAIRY_TALE/texts/bn_00187346n_The Dove.txt\n",
            "-Loaded: ./FAIRY_TALE/texts/bn_00187442n_Corvetto.txt\n",
            "-Loaded: ./FAIRY_TALE/texts/bn_00608721n_Sun, Moon, and Talia.txt\n",
            "-Loaded: ./FAIRY_TALE/texts/bn_00790303n_Childe Rowland.txt\n",
            "-Loaded: ./FAIRY_TALE/texts/bn_01593735n_Town Musicians of Bremen.txt\n",
            "-Loaded: ./FAIRY_TALE/texts/bn_01899260n_The Raven.txt\n",
            "-Loaded: ./FAIRY_TALE/texts/bn_02154461n_Herr Korbes.txt\n",
            "-Loaded: ./FAIRY_TALE/texts/bn_02173122n_Thumbelina.txt\n",
            "-Loaded: ./FAIRY_TALE/texts/bn_02238889n_The Nightingale.txt\n",
            "-Loaded: ./FAIRY_TALE/texts/bn_02277659n_The Ugly Duckling.txt\n",
            "-Loaded: ./FAIRY_TALE/texts/bn_02442758n_Jack the Giant Killer.txt\n",
            "-Loaded: ./FAIRY_TALE/texts/bn_03300215n_The Wonderful Birch.txt\n",
            "-Loaded: ./FAIRY_TALE/texts/bn_03301753n_Cap-o_-Rushes.txt\n",
            "-Loaded: ./FAIRY_TALE/texts/bn_03326399n_The Story of Pretty Goldilocks.txt\n",
            "-Loaded: ./FAIRY_TALE/texts/bn_03329137n_Soria Moria Castle.txt\n",
            "-Loaded: ./FAIRY_TALE/texts/bn_03329494n_The Cat on the Dovrefjell.txt\n",
            "-Loaded: ./FAIRY_TALE/texts/bn_03331799n_The Language of the Birds.txt\n",
            "-Loaded: ./FAIRY_TALE/texts/bn_03332887n_Petrosinella.txt\n",
            "-Loaded: ./FAIRY_TALE/texts/bn_03364076n_Princess Rosette.txt\n",
            "-Loaded: ./FAIRY_TALE/texts/bn_03377704n_Gold-Tree and Silver-Tree.txt\n",
            "-Loaded: ./FAIRY_TALE/texts/bn_03393628n_Cannetella.txt\n",
            "-Loaded: ./FAIRY_TALE/texts/bn_03505587n_The Ram.txt\n",
            "-Loaded: ./FAIRY_TALE/texts/bn_03526381n_Ferdinand the Faithful and Ferdinand the Unfaithful.txt\n",
            "-Loaded: ./FAIRY_TALE/texts/bn_03550932n_The She-bear.txt\n",
            "-Loaded: ./FAIRY_TALE/texts/bn_03661112n_The Tale of Tsar Saltan.txt\n",
            "-Loaded: ./FAIRY_TALE/texts/bn_03729159n_Pintosmalto.txt\n",
            "-Loaded: ./FAIRY_TALE/texts/bn_03729229n_The Love for Three Oranges.txt\n",
            "-Loaded: ./FAIRY_TALE/texts/bn_03730572n_The Months.txt\n",
            "-Loaded: ./FAIRY_TALE/texts/bn_03813035n_Snow-White and Rose-Red.txt\n",
            "-Loaded: ./FAIRY_TALE/texts/bn_03884049n_The Enchanted Snake.txt\n",
            "-Loaded: ./FAIRY_TALE/texts/bn_03980496n_The Jew Among Thorns.txt\n",
            "-Loaded: ./FAIRY_TALE/texts/bn_14140242n_The Frog Prince.txt\n",
            "-Loaded: ./FAIRY_TALE/texts/bn_14145915n_The Boy Who Had an Eating Match with a Troll.txt\n",
            "-Loaded: ./FAIRY_TALE/texts/bn_15417261n_Jack and the Beanstalk.txt\n",
            "-Loaded: ./FAIRY_TALE/texts/bn_17385267n_The Brave Little Tailor.txt\n",
            "-Loaded: ./FAIRY_TALE/texts/bn_21706209n_Bluebeard.txt\n",
            "-Loaded: ./FAIRY_TALE/texts/bn_21706329n_Cupid and Psyche.txt\n",
            "-Loaded: ./SHORT_STORY/texts/bn_00183025n_Rab and his Friends.txt\n",
            "-Loaded: ./SHORT_STORY/texts/bn_00210737n_Alyosha the Pot.txt\n",
            "-Loaded: ./SHORT_STORY/texts/bn_00283865n_The Great Carbuncle.txt\n",
            "-Loaded: ./SHORT_STORY/texts/bn_00288807n_The Man of Adamant.txt\n",
            "-Loaded: ./SHORT_STORY/texts/bn_00645621n_A Descent into the Maelström.txt\n",
            "-Loaded: ./SHORT_STORY/texts/bn_00655138n_Thubway Tham_s Inthane Moment.txt\n",
            "-Loaded: ./SHORT_STORY/texts/bn_00663751n_The Facts in the Case of M. Valdemar.txt\n",
            "-Loaded: ./SHORT_STORY/texts/bn_00955099n_The Nameless City.txt\n",
            "-Loaded: ./SHORT_STORY/texts/bn_01190519n_The Emperor_s New Clothes.txt\n",
            "-Loaded: ./SHORT_STORY/texts/bn_01304399n_The Music of Erich Zann.txt\n",
            "-Loaded: ./SHORT_STORY/texts/bn_01304444n_The Cats of Ulthar.txt\n",
            "-Loaded: ./SHORT_STORY/texts/bn_01323408n_The Damned Thing.txt\n",
            "-Loaded: ./SHORT_STORY/texts/bn_01425478n_The Vengeance of Nitocris.txt\n",
            "-Loaded: ./SHORT_STORY/texts/bn_01436803n_The Answer.txt\n",
            "-Loaded: ./SHORT_STORY/texts/bn_01521190n_Claude Gueux.txt\n",
            "-Loaded: ./SHORT_STORY/texts/bn_01630488n_The Haunter of the Dark.txt\n",
            "-Loaded: ./SHORT_STORY/texts/bn_01644148n_Pickman_s Model.txt\n",
            "-Loaded: ./SHORT_STORY/texts/bn_01745953n_An Occurrence at Owl Creek Bridge.txt\n",
            "-Loaded: ./SHORT_STORY/texts/bn_01789382n_Rip Van Winkle.txt\n",
            "-Loaded: ./SHORT_STORY/texts/bn_02107348n_The Isle of Voices.txt\n",
            "-Loaded: ./SHORT_STORY/texts/bn_02201468n_The Doom That Came to Sarnath.txt\n",
            "-Loaded: ./SHORT_STORY/texts/bn_02275371n_The Outsider.txt\n",
            "-Loaded: ./SHORT_STORY/texts/bn_02413967n_The Black Cat.txt\n",
            "-Loaded: ./SHORT_STORY/texts/bn_02439324n_Micromégas.txt\n",
            "-Loaded: ./SHORT_STORY/texts/bn_02464343n_From Beyond.txt\n",
            "-Loaded: ./SHORT_STORY/texts/bn_02485046n_Brooksmith.txt\n",
            "-Loaded: ./SHORT_STORY/texts/bn_02502064n_Facino Cane.txt\n",
            "-Loaded: ./SHORT_STORY/texts/bn_02767919n_To Whom This May Come.txt\n",
            "-Loaded: ./SHORT_STORY/texts/bn_02975525n_Rothschild_s Violin.txt\n",
            "-Loaded: ./SHORT_STORY/texts/bn_03079213n_The Quest of Iranon.txt\n",
            "-Loaded: ./SHORT_STORY/texts/bn_03149445n_The Tree.txt\n",
            "-Loaded: ./SHORT_STORY/texts/bn_03149511n_The Tomb.txt\n",
            "-Loaded: ./SHORT_STORY/texts/bn_03176704n_The Gift of the Magi.txt\n",
            "-Loaded: ./SHORT_STORY/texts/bn_03214242n_Polaris.txt\n",
            "-Loaded: ./SHORT_STORY/texts/bn_03220342n_The Rats in the Walls.txt\n",
            "-Loaded: ./SHORT_STORY/texts/bn_03280831n_The Hound.txt\n",
            "-Loaded: ./SHORT_STORY/texts/bn_03297228n_.007.txt\n",
            "-Loaded: ./SHORT_STORY/texts/bn_03341625n_The Fall of the House of Usher.txt\n",
            "-Loaded: ./SHORT_STORY/texts/bn_03396386n_Hypnos.txt\n",
            "-Loaded: ./SHORT_STORY/texts/bn_03396420n_The Bet.txt\n",
            "-Loaded: ./SHORT_STORY/texts/bn_03419493n_A Country Doctor.txt\n",
            "-Loaded: ./SHORT_STORY/texts/bn_03441968n_The Picture in the House.txt\n",
            "-Loaded: ./SHORT_STORY/texts/bn_03453004n_The Lagoon.txt\n",
            "-Loaded: ./SHORT_STORY/texts/bn_03461916n_The Voice in the Night.txt\n",
            "-Loaded: ./SHORT_STORY/texts/bn_03518160n_The Descendant.txt\n",
            "-Loaded: ./SHORT_STORY/texts/bn_03520887n_The Evil Clergyman.txt\n",
            "-Loaded: ./SHORT_STORY/texts/bn_03520895n_In the Vault.txt\n",
            "-Loaded: ./SHORT_STORY/texts/bn_03520903n_Ibid.txt\n",
            "-Loaded: ./SHORT_STORY/texts/bn_03520905n_Old Bugs.txt\n",
            "-Loaded: ./SHORT_STORY/texts/bn_03520913n_The Street.txt\n",
            "-Loaded: ./SHORT_STORY/texts/bn_03520916n_Sweet Ermengarde.txt\n",
            "-Loaded: ./SHORT_STORY/texts/bn_03520917n_The Transition of Juan Romero.txt\n",
            "-Loaded: ./SHORT_STORY/texts/bn_03655527n_A Pail of Air.txt\n",
            "-Loaded: ./SHORT_STORY/texts/bn_03729761n_The Legend of Sleepy Hollow.txt\n"
          ]
        }
      ],
      "source": [
        "load_mod = \"local\"\n",
        "fairy_tale_path = \"./FAIRY_TALE/texts\"\n",
        "short_story_path = \"./SHORT_STORY/texts\"\n",
        "\n",
        "if load_mod == \"drive\":\n",
        "  drive.mount(\"/content/drive\", force_remount=True)\n",
        "  %cd '/content/drive/MyDrive/Colab Notebooks/Narrative understanding and storytelling/homework_1a/FairySum'\n",
        "  %ls\n",
        "else: # load locally\n",
        "  from google.colab import files\n",
        "  files.upload()\n",
        "\n",
        "  file_name = \"./FairySum.zip\"\n",
        "  with ZipFile(file_name, 'r') as zip_file:\n",
        "    zip_file.extractall()\n",
        "  %cd FairySum\n",
        "\n",
        "fairy_tale_textPaths  = [fairy_tale_path + \"/\" + text for text in sorted(os.listdir(fairy_tale_path))]\n",
        "short_Story_textsPaths = [short_story_path + \"/\" + text for text in sorted(os.listdir(short_story_path))]\n",
        "\n",
        "# merge paths about fairy tales and short stories\n",
        "full_list_paths = [*fairy_tale_textPaths,*short_Story_textsPaths]\n",
        "\n",
        "for x in full_list_paths: print(\"-Loaded: {}\".format(x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bHpTIAXJ-3Of"
      },
      "source": [
        "## Manage data "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "fnbd0ouS_c2t"
      },
      "outputs": [],
      "source": [
        "\"function to extract the raw text from a .txt file\"\n",
        "def text_from_book(path):\n",
        "  with open(path,'r') as file:\n",
        "    text = [line for line in file.readlines()]\n",
        "    file.close()\n",
        "    return text\n",
        "\n",
        "\"function to retrieve the span from the list of sentences of a document, start and end char position\"\n",
        "# note: start and end position parameters are in reference to char position in the whole document\n",
        "# use it to find the correct sentence, and then remove this off-set to have the local index for\n",
        "# characters in the single sentence.\n",
        "def get_span(doc_sentences, start_pos, end_pos, verbose = False):\n",
        "  idx_seek = 0\n",
        "  idx_start = 0                                 # first char index of the sentence \n",
        "  idx_end = len(doc_sentences[idx_seek])        # last char index of the sentence \n",
        "  if verbose: print(\"| {} | {} | {} |\".format(idx_seek,idx_start,idx_end))\n",
        "  while(not(start_pos >= idx_start and end_pos <= idx_end)):\n",
        "    idx_seek += 1                               # update seek index for the sentences\n",
        "    idx_start = idx_end                         # update first and last index of the actual sentence \n",
        "    try:\n",
        "      idx_end += len(doc_sentences[idx_seek])\n",
        "    except IndexError as e:\n",
        "      print(f\"{e}\")\n",
        "      return None\n",
        "\n",
        "    if verbose: print(\"| {} | {} | {} |\".format(idx_seek,idx_start,idx_end))\n",
        "  \n",
        "  # remove off-set global char\n",
        "  idx_1 = start_pos-idx_end\n",
        "  idx_2 = end_pos-idx_end\n",
        "\n",
        "  # get span and return \n",
        "  span = doc_sentences[idx_seek][idx_1:idx_2]\n",
        "  if verbose: print(f\"Found {span}\")\n",
        "  return span\n",
        "\n",
        "\"function to retrieve the sentence given the global start and end indices of the char\"\n",
        "def get_sentence(doc_sentences, start_pos, end_pos, verbose = False):\n",
        "  idx_seek = 0\n",
        "  idx_start = 0                                 # first char index of the sentence \n",
        "  idx_end = len(doc_sentences[idx_seek])        # last char index of the sentence \n",
        "  if verbose: print(\"| {} | {} | {} |\".format(idx_seek,idx_start,idx_end))\n",
        "  while(not(start_pos >= idx_start and end_pos <= idx_end)):\n",
        "    idx_seek += 1                               # update seek index for the sentences\n",
        "    idx_start = idx_end                         # update first and last index of the actual sentence \n",
        "    try:\n",
        "      idx_end += len(doc_sentences[idx_seek])\n",
        "    except IndexError as e:\n",
        "      print(f\"{e}\")\n",
        "      return None\n",
        "\n",
        "    if verbose: print(\"| {} | {} | {} |\".format(idx_seek,idx_start,idx_end))\n",
        "  \n",
        "  return doc_sentences[idx_seek]\n",
        "  \n",
        "\"\"\"\n",
        "function to extract just the labels of interest in NER\n",
        "two modes: \n",
        "- LOC: just take the entities with the LOC label\n",
        "- LOC: include as location also FAC and GPE entities, besides LOC.\n",
        "\"\"\"\n",
        "def get_labels(doc, mode=\"LOC+\"):        # what to include in LOC?\n",
        "  full_ents = doc.ents  # the full set of annoted spans\n",
        "  plo_ents = [] # the annoted spans with categories: {PER,LOC,ORG}\n",
        "  for ent in full_ents:\n",
        "\n",
        "    if ent.label_ == \"PERSON\": ent.label_ = \"PER\" # just rename the label\n",
        "       \n",
        "    if mode == \"LOC+\":\n",
        "      if ent.label_ in [\"PER\",\"LOC\",\"ORG\"]:  \n",
        "        plo_ents.append(ent)\n",
        "      if ent.label_ in [\"FAC\",\"GPE\"]: # chosen to include also facilities and Geopolitical entities\n",
        "        ent.label_ = \"LOC\"\n",
        "        plo_ents.append(ent)\n",
        "    else:\n",
        "      if ent.label_ in [\"PER\",\"LOC\",\"ORG\"]:  \n",
        "        plo_ents.append(ent)\n",
        "  return plo_ents\n",
        "\n",
        "\"\"\" priting table not on the line \"\"\"\n",
        "def print_table(table):\n",
        "  for i,(k,v) in enumerate(table.items()):\n",
        "    print(\"{:<4} K: {:<30} V: {:<30}\".format(str(i)+\")\",str(k),str(v)))\n",
        "\n",
        "\"\"\" priting list not on the line \"\"\"\n",
        "def print_list(list):\n",
        "  for i,elem in enumerate(list):\n",
        "    print(\"{:<4} {:<30}\".format(str(i)+\")\",str(elem)))\n",
        "\n",
        "\"\"\" function to filter and preprocessing strings for the comparison in the external identification \"\"\"\n",
        "\n",
        "def reduce_span(span):\n",
        "  span = span.lower().strip() # make words lower, and exclude extra spaces at begin/end\n",
        "  filtered_words = [word for word in span.split() if word not in stopwords.words('english')]# filter excluding english stop words\n",
        "  return \" \".join(filtered_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yjxeyEcj5sPv"
      },
      "source": [
        "# Functions for optional tasks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dh6Hdg3K50oU"
      },
      "source": [
        "## Unique name identifying\n",
        "## (optional 1) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "PLBtqzrL6Fb-"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "sorting functions\n",
        "\"\"\"\n",
        "def first_criterion(item):\n",
        "  # take the values in the items, so [1] and takes the indices [0]\n",
        "  value_idxs = item[1][0]\n",
        "  # and return the first index (first appearance)\n",
        "  return value_idxs[0]\n",
        "\n",
        "def longer_criterion(item):\n",
        "  # return length span\n",
        "  key = item[0]\n",
        "  return -len(key) # use minus to reverse the sorting\n",
        "\n",
        "def mf_criterion(item):\n",
        "  # return length of the indices list\n",
        "  value_idxs = item[1][0] \n",
        "  return -len(value_idxs)\n",
        "\n",
        "# map: mode -> sorting criterion\n",
        "sorting_criterions = {\"first\":first_criterion ,\"longer\":longer_criterion, \"mf\":mf_criterion}\n",
        "\n",
        "\n",
        "\"function to check if a Nemed entity is contained in more than another Named Entity in y_doc\"\n",
        "def uniqueContainment(list_keys, key_a, debug = False):\n",
        "  try:\n",
        "    list_keys.remove(key_a)\n",
        "    count = 0\n",
        "    for key_b in list_keys:\n",
        "      if   key_b.__contains__(key_a): \n",
        "        count += 1\n",
        "        if debug: print(f\"{key_a} <-> {key_b} | count: {count}\")\n",
        "    if count <= 1:\n",
        "      return True\n",
        "    else:\n",
        "      return False\n",
        "  except Exception as e:\n",
        "    print(e + f\" with the following instances, list keys: {list_keys}, key_check: {key_a}\")\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "merge dictionary entries for the same Named Entity\n",
        "\"\"\"\n",
        "def merge_items(table, debug = False):\n",
        "\n",
        "  if debug: print(\"------------------start---------------------\\n\")\n",
        "  edited = False # default False, stays false iff no edits while scaning the whole set of entries \n",
        "  to_delete = [] # list of entries to delete \n",
        "\n",
        "  for i,(k,v) in enumerate(table.items()):\n",
        "    j_elems= list(range(len(table.items())))\n",
        "    if debug: print(f\"1st entry: {i}\")\n",
        "    j_elems.remove(i)\n",
        "    if debug: print(f\"2nd entries: {j_elems}\")\n",
        "\n",
        "    for j in j_elems:\n",
        "      j_keys = list(table.keys())\n",
        "      j_values = list(table.values())\n",
        "\n",
        "      # here you can change the policy for the merging of entries\n",
        "      # in this case is a simple contains condition\n",
        "\n",
        "      if (k.__contains__(j_keys[j]) and uniqueContainment(copy.deepcopy(j_keys),j_keys[j])) \\\n",
        "          or (j_keys[j].__contains__(k) and uniqueContainment(copy.deepcopy(j_keys),k)):\n",
        "\n",
        "        if debug and k.__contains__(j_keys[j]): print(f\"{k} <- {j_keys[j]}\")\n",
        "        if debug and j_keys[j].__contains__(k): print(f\"{k} -> {j_keys[j]}\")\n",
        "        edited = True\n",
        "        v[0].extend(j_values[j][0])\n",
        "        to_delete.append(j_keys[j])\n",
        "      \n",
        "\n",
        "    if debug: print(f\"entries to delete: {to_delete}\")\n",
        "\n",
        "    for key in to_delete:\n",
        "      del table[key]\n",
        "    # exit if we have a merge, it's needed to restart the control\n",
        "    if edited: break\n",
        "  if debug: print(\"------------------end---------------------\\n\")\n",
        "\n",
        "  return edited\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "function to select unique name for same named entities\n",
        "modes:\n",
        "first : take as unique name the first that is encountered in the results\n",
        "longer: take as unique name the longer to express the same Named Entity\n",
        "mf    : take as unique name the one which is most frequent to represent the same Named Entity\n",
        "\"\"\"\n",
        "def set_uniqueNames(y_doc, sents_doc, mode=\"longer\", debug= False):\n",
        "  \n",
        "  # create a deep copy of the NER results for the document\n",
        "  y_doc_unique = copy.deepcopy(y_doc)\n",
        "\n",
        "  # create a map: Named Entity for check -> [indices in y_doc], original Named Entity\n",
        "  map_ent_idx = {}\n",
        "  for idx, y_row in enumerate(y_doc):\n",
        "\n",
        "    span_original = get_span(sents_doc,y_row[1], y_row[2])\n",
        "    span_check = span_original.strip().lower()  # to exclude too silly cases \n",
        "    if debug: print(f\"{span_original}, {span_check}\")\n",
        "    if span_check in map_ent_idx.keys():\n",
        "      value = map_ent_idx[span_check]\n",
        "      value[0].append(idx)\n",
        "    else:\n",
        "      map_ent_idx[span_check] = [[idx],span_original]\n",
        "    \n",
        "  # sort the dictionary \n",
        "  criterion = sorting_criterions[mode]\n",
        "  map_ent_idx = dict(sorted(map_ent_idx.items(), key = criterion))\n",
        "\n",
        "  \n",
        "  edited = True\n",
        "  # merge entries if substring\n",
        "  # do-while loop\n",
        "  if len(map_ent_idx) > 1:\n",
        "     edited = merge_items(map_ent_idx)\n",
        "  while(edited and len(map_ent_idx) > 1):\n",
        "    edited =  merge_items(map_ent_idx)\n",
        "  \n",
        "  # add the unique name in the list of document's output\n",
        "  for k,v in map_ent_idx.items():\n",
        "    for idx in v[0]:\n",
        "      if len(y_doc_unique[idx]) == 4:\n",
        "        y_doc_unique[idx].insert(4,v[1]) # unique name is the 5th element in the row\n",
        "\n",
        "  if debug: print_list(y_doc_unique)\n",
        "\n",
        "  return  map_ent_idx,y_doc_unique"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDBmLW3P507Z"
      },
      "source": [
        "## External identification\n",
        "##(optional 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RRNFs6rXSJAr"
      },
      "source": [
        "### Babelnet\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "mWGkrg876FLX"
      },
      "outputs": [],
      "source": [
        "\"\"\" \n",
        "1st function to retrieve babalnet IDs\n",
        "\"\"\"\n",
        "def query_serverUSEA(text, debug = False):\n",
        "\n",
        "  if debug: startTime = time.time()\n",
        "\n",
        "  def extract_synsetsIDs(response, text):\n",
        "    words_ids = [] # elements -> (word,id)\n",
        "    words = text.split()\n",
        "    \n",
        "    list_wsd = response[\"response\"][\"texts\"][0][\"annotations\"][\"wsd\"]\n",
        "    for elem in list_wsd:\n",
        "      word = words[elem[\"start\"]]\n",
        "      bn_id = elem[\"features\"][\"synset\"]\n",
        "      print(f\"word: {word} | babelnet id:{bn_id}\")\n",
        "      words_ids.append((word,bn_id))\n",
        "      \n",
        "    return words_ids\n",
        "  \n",
        "  # sometimes the server gives an error as response\n",
        "  response = requests.post(\n",
        "      \"https://nlp.uniroma1.it/usea/api\", json={\"type\": \"text\", \"content\": text}\n",
        "  ).json()\n",
        "  \n",
        "  if debug:\n",
        "    endTime = time.time()\n",
        "    print(\"time elapsed for Entity Linking {}\".format(endTime-startTime))\n",
        "    print(json.dumps(response, indent=2))\n",
        "\n",
        "  # get babalnet_IDs\n",
        "  return extract_synsetsIDs(response, text)\n",
        "\n",
        "\n",
        "\"\"\" \n",
        "2nd function to retrieve babalnet IDs\n",
        "\"\"\"\n",
        "def query_babelfy(text, debug = False):\n",
        "\n",
        "  span2id = [] # set of tuple with (span,babelnet_id)\n",
        "\n",
        "  service_url = 'https://babelfy.io/v1/disambiguate'\n",
        "\n",
        "  lang = 'EN'\n",
        "\n",
        "  # for a complete execution substiture here e key with no queries limit\n",
        "  key  = 'd8bea0c4-527f-41e4-9fb1-50a17dacbfe6'\n",
        "\n",
        "  params = {\n",
        "    'text' : text,\n",
        "    'lang' : lang,\n",
        "    'key'  : key\n",
        "  }\n",
        "\n",
        "  url = service_url + '?' + urllib.parse.urlencode(params)\n",
        "  request = urllib2.Request(url)\n",
        "  request.add_header('Accept-encoding', 'gzip')\n",
        "  response = urllib2.urlopen(request)\n",
        "\n",
        "  if response.info().get('Content-Encoding') == 'gzip':\n",
        "    buf = io.BytesIO( response.read())\n",
        "    f = gzip.GzipFile(fileobj=buf)\n",
        "    data = json.loads(f.read())\n",
        "\n",
        "    if debug: print(data)\n",
        "    # retrieving data\n",
        "    for result in data:\n",
        "                  # retrieving token fragment\n",
        "                  tokenFragment = result.get('tokenFragment')\n",
        "                  tfStart = tokenFragment.get('start')\n",
        "                  tfEnd = tokenFragment.get('end')\n",
        "                  if debug:print(str(tfStart) + \"\\t\" + str(tfEnd))\n",
        "\n",
        "                  # retrieving char fragment\n",
        "                  charFragment = result.get('charFragment')\n",
        "                  cfStart = charFragment.get('start')\n",
        "                  cfEnd = charFragment.get('end')\n",
        "                  if debug: print(str(cfStart) + \"\\t\" + str(cfEnd))\n",
        "                  \n",
        "                  # retrieving BabelSynset ID\n",
        "                  synsetId = result.get('babelSynsetID')\n",
        "                  \n",
        "                  span = \" \".join(text.split()[tfStart:tfEnd+1])\n",
        "                  \n",
        "                  if debug:\n",
        "                    print(synsetId)\n",
        "                    print(\"inserting the following tuple in the results: ({},{})\".format \\\n",
        "                          (span,synsetId))\n",
        "                  \n",
        "                  span2id.append((span,synsetId))\n",
        "\n",
        "  return span2id\n",
        "\n",
        "\"\"\"\n",
        "get babelnet IDs, two different applications can be chosen for this:\n",
        "- Babelefy\n",
        "- Usea\n",
        "(use Babelfy)\n",
        "crucial difference from the EL with wikidata: \n",
        "limited number of request (free account with 1000 coins), so once found a match for a certain span\n",
        "use the same IDs for all the repetition of that span.\n",
        "this obviously reduce the accuracy, but makes the execution feasible.\n",
        "\"\"\"\n",
        "def get_babelIDs(sentences, y_doc, map_ent_idx, ed_match = 2,  mode =\"Babelefy\", verbose = False):\n",
        "  # retrive from the map the rows of y_doc with same Named Entity\n",
        "\n",
        "  counter = 0\n",
        "  for k,v in map_ent_idx.items():\n",
        "    \n",
        "    indices_y = v[0]\n",
        "    is_match = False # boolean to handle less queries\n",
        "    tmp_id = \"\"\n",
        "    tmp_span = \"\"\n",
        "    for idx_y in indices_y:\n",
        "      y_row = y_doc[idx_y]\n",
        "      sentence = get_sentence(sentences,y_row[1],y_row[2])\n",
        "\n",
        "      # get the ID of the knowledge base\n",
        "      if mode == \"Babelefy\":\n",
        "        try:\n",
        "          \n",
        "          span2id = query_babelfy(sentence)\n",
        "          print(counter)\n",
        "        except Exception as e:\n",
        "          print(\"limit exceeded, counter queries: {}\".format(counter))\n",
        "          return \n",
        "        counter += 1\n",
        "      else:\n",
        "        span2id = query_serverUSEA(sentence)\n",
        "      \n",
        "      # check if there's a linked entity that match the actual Named Entity\n",
        "      for (new_span,new_babel_id) in span2id:\n",
        "        if verbose: print(f\"{v[1]} | {new_span} | {new_babel_id}\")\n",
        "        # filtering span for mathing\n",
        "        s1 = reduce_span(v[1])\n",
        "        s2 = reduce_span(new_span)\n",
        "\n",
        "        # first prefer to choose as match the span with an edit distance lower than ed_match\n",
        "        # if you find one, break the loop\n",
        "        if editdistance.eval(s1,s2) <=  ed_match:\n",
        "          is_match = True\n",
        "          tmp_span = new_span\n",
        "          tmp_id = str(new_babel_id)\n",
        "          break\n",
        " \n",
        "      if is_match: break\n",
        "    \n",
        "    if (tmp_id!= \"\") and (tmp_span != \"\"):\n",
        "      for idx_y in indices_y:\n",
        "        y_row = y_doc[idx_y] \n",
        "        if verbose: print(f\"inserting: {v[1]} | {tmp_span} | {tmp_id}\")\n",
        "        y_row.insert(5,tmp_id) #add the last element in the vector\n",
        "\n",
        "  return"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OnhIsbsASQg9"
      },
      "source": [
        "### Wikidata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "43hZYZSFGX6I"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "function that retrive wikidata IDs using a spacy pipeline with entity-linker\n",
        "ed_match: parameter used to validate a new span equal to the ones found through NER \n",
        "\"\"\"\n",
        "def get_wikidataID(el_model, sentences, y_doc, map_ent_idx, ed_match = 2, verbose = False):\n",
        "  # retrive from the map the rows of y_doc with same Named Entity\n",
        "  for k,v in map_ent_idx.items():\n",
        "    indices_y = v[0]\n",
        "\n",
        "    for idx_y in indices_y:\n",
        "\n",
        "      y_row = y_doc[idx_y]\n",
        "      # get the ID of the knowledge base\n",
        "      sentence = get_sentence(sentences,y_row[1],y_row[2])\n",
        "\n",
        "      # sometimes there're white spaces at the end of the sentence that causes error\n",
        "      # handle this problem here\n",
        "      try: \n",
        "        doc = el_model(sentence)\n",
        "      except Exception as e:\n",
        "        sentence = sentence.rstrip()\n",
        "        doc = el_model(sentence)\n",
        "\n",
        "      # check if there's a linked entity that match the actual Named Entity\n",
        "      for linked_entity in doc._.linkedEntities:\n",
        "        new_span = str(linked_entity.get_span())\n",
        "        new_wikidata_id = \"Q\"+str(linked_entity.get_id())\n",
        "        if verbose: print(f\"{v[1]} | {new_span} | {new_wikidata_id}\")\n",
        "        # filtering span for mathing\n",
        "        s1 = reduce_span(v[1])\n",
        "        s2 = reduce_span(new_span)\n",
        "\n",
        "        # first prefer to choose as match the span with an edit distance lower than ed_match\n",
        "        # if you find one, break the loop\n",
        "        if editdistance.eval(s1,s2) <=  ed_match:\n",
        "          if verbose: print(f\"inserting: {v[1]} | {new_span} | {new_wikidata_id}\")\n",
        "          y_row.insert(5,new_wikidata_id) #add the last element in the vector\n",
        "          break\n",
        "          \n",
        "  return"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmakS9MFZcAc"
      },
      "source": [
        "# Named Entity Recognition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U4FEFFNutxCQ"
      },
      "source": [
        "## Initialization "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "8Pv22Calz2Pi"
      },
      "outputs": [],
      "source": [
        "# loading of the trained pipeline from spacy for NER\n",
        "nlp = spacy.load(PIPELINES_TYPE[SIZE_IDX])\n",
        "\n",
        "# loading of the trained pipeline from spacy for EL\n",
        "if EL_RESOURCE == \"wikidata\":\n",
        "  # you can't use transformer models including the default entity linker in the pipeline\n",
        "  nlp_el = spacy.load(\"en_core_web_md\")\n",
        "  nlp_el.add_pipe(\"entityLinker\", last=True) # for this the restart of the runtime is needed \n",
        "\n",
        "# variables to toogle verbose exectution of the main loop\n",
        "verbose = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHVT2ds4FmRh"
      },
      "source": [
        "## Named Entities extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "vz01dyLgoVfP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e82a2b2c-c140-47b3-f6ed-77db8e8015bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting Named Entity in the text n°1: The Old Dame and her Hen.txt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/amp/autocast_mode.py:198: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
            "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Extracting Named Entity in the text n°2: The Dove.txt\n",
            "\n",
            "Extracting Named Entity in the text n°3: Corvetto.txt\n",
            "\n",
            "Extracting Named Entity in the text n°4: Sun, Moon, and Talia.txt\n",
            "\n",
            "Extracting Named Entity in the text n°5: Childe Rowland.txt\n",
            "\n",
            "Extracting Named Entity in the text n°6: Town Musicians of Bremen.txt\n",
            "\n",
            "Extracting Named Entity in the text n°7: The Raven.txt\n",
            "\n",
            "Extracting Named Entity in the text n°8: Herr Korbes.txt\n",
            "\n",
            "Extracting Named Entity in the text n°9: Thumbelina.txt\n",
            "\n",
            "Extracting Named Entity in the text n°10: The Nightingale.txt\n",
            "\n",
            "Extracting Named Entity in the text n°11: The Ugly Duckling.txt\n",
            "\n",
            "Extracting Named Entity in the text n°12: Jack the Giant Killer.txt\n",
            "\n",
            "Extracting Named Entity in the text n°13: The Wonderful Birch.txt\n",
            "\n",
            "Extracting Named Entity in the text n°14: Cap-o -Rushes.txt\n",
            "\n",
            "Extracting Named Entity in the text n°15: The Story of Pretty Goldilocks.txt\n",
            "\n",
            "Extracting Named Entity in the text n°16: Soria Moria Castle.txt\n",
            "\n",
            "Extracting Named Entity in the text n°17: The Cat on the Dovrefjell.txt\n",
            "\n",
            "Extracting Named Entity in the text n°18: The Language of the Birds.txt\n",
            "\n",
            "Extracting Named Entity in the text n°19: Petrosinella.txt\n",
            "\n",
            "Extracting Named Entity in the text n°20: Princess Rosette.txt\n",
            "\n",
            "Extracting Named Entity in the text n°21: Gold-Tree and Silver-Tree.txt\n",
            "\n",
            "Extracting Named Entity in the text n°22: Cannetella.txt\n",
            "\n",
            "Extracting Named Entity in the text n°23: The Ram.txt\n",
            "\n",
            "Extracting Named Entity in the text n°24: Ferdinand the Faithful and Ferdinand the Unfaithful.txt\n",
            "\n",
            "Extracting Named Entity in the text n°25: The She-bear.txt\n",
            "\n",
            "Extracting Named Entity in the text n°26: The Tale of Tsar Saltan.txt\n",
            "\n",
            "Extracting Named Entity in the text n°27: Pintosmalto.txt\n",
            "\n",
            "Extracting Named Entity in the text n°28: The Love for Three Oranges.txt\n",
            "\n",
            "Extracting Named Entity in the text n°29: The Months.txt\n",
            "\n",
            "Extracting Named Entity in the text n°30: Snow-White and Rose-Red.txt\n",
            "\n",
            "Extracting Named Entity in the text n°31: The Enchanted Snake.txt\n",
            "\n",
            "Extracting Named Entity in the text n°32: The Jew Among Thorns.txt\n",
            "\n",
            "Extracting Named Entity in the text n°33: The Frog Prince.txt\n",
            "\n",
            "Extracting Named Entity in the text n°34: The Boy Who Had an Eating Match with a Troll.txt\n",
            "\n",
            "Extracting Named Entity in the text n°35: Jack and the Beanstalk.txt\n",
            "\n",
            "Extracting Named Entity in the text n°36: The Brave Little Tailor.txt\n",
            "\n",
            "Extracting Named Entity in the text n°37: Bluebeard.txt\n",
            "\n",
            "Extracting Named Entity in the text n°38: Cupid and Psyche.txt\n",
            "\n",
            "Extracting Named Entity in the text n°39: Rab and his Friends.txt\n",
            "\n",
            "Extracting Named Entity in the text n°40: Alyosha the Pot.txt\n",
            "\n",
            "Extracting Named Entity in the text n°41: The Great Carbuncle.txt\n",
            "\n",
            "Extracting Named Entity in the text n°42: The Man of Adamant.txt\n",
            "\n",
            "Extracting Named Entity in the text n°43: A Descent into the Maelström.txt\n",
            "\n",
            "Extracting Named Entity in the text n°44: Thubway Tham s Inthane Moment.txt\n",
            "\n",
            "Extracting Named Entity in the text n°45: The Facts in the Case of M. Valdemar.txt\n",
            "\n",
            "Extracting Named Entity in the text n°46: The Nameless City.txt\n",
            "\n",
            "Extracting Named Entity in the text n°47: The Emperor s New Clothes.txt\n",
            "\n",
            "Extracting Named Entity in the text n°48: The Music of Erich Zann.txt\n",
            "\n",
            "Extracting Named Entity in the text n°49: The Cats of Ulthar.txt\n",
            "\n",
            "Extracting Named Entity in the text n°50: The Damned Thing.txt\n",
            "\n",
            "Extracting Named Entity in the text n°51: The Vengeance of Nitocris.txt\n",
            "\n",
            "Extracting Named Entity in the text n°52: The Answer.txt\n",
            "\n",
            "Extracting Named Entity in the text n°53: Claude Gueux.txt\n",
            "\n",
            "Extracting Named Entity in the text n°54: The Haunter of the Dark.txt\n",
            "\n",
            "Extracting Named Entity in the text n°55: Pickman s Model.txt\n",
            "\n",
            "Extracting Named Entity in the text n°56: An Occurrence at Owl Creek Bridge.txt\n",
            "\n",
            "Extracting Named Entity in the text n°57: Rip Van Winkle.txt\n",
            "\n",
            "Extracting Named Entity in the text n°58: The Isle of Voices.txt\n",
            "\n",
            "Extracting Named Entity in the text n°59: The Doom That Came to Sarnath.txt\n",
            "\n",
            "Extracting Named Entity in the text n°60: The Outsider.txt\n",
            "\n",
            "Extracting Named Entity in the text n°61: The Black Cat.txt\n",
            "\n",
            "Extracting Named Entity in the text n°62: Micromégas.txt\n",
            "\n",
            "Extracting Named Entity in the text n°63: From Beyond.txt\n",
            "\n",
            "Extracting Named Entity in the text n°64: Brooksmith.txt\n",
            "\n",
            "Extracting Named Entity in the text n°65: Facino Cane.txt\n",
            "\n",
            "Extracting Named Entity in the text n°66: To Whom This May Come.txt\n",
            "\n",
            "Extracting Named Entity in the text n°67: Rothschild s Violin.txt\n",
            "\n",
            "Extracting Named Entity in the text n°68: The Quest of Iranon.txt\n",
            "\n",
            "Extracting Named Entity in the text n°69: The Tree.txt\n",
            "\n",
            "Extracting Named Entity in the text n°70: The Tomb.txt\n",
            "\n",
            "Extracting Named Entity in the text n°71: The Gift of the Magi.txt\n",
            "\n",
            "Extracting Named Entity in the text n°72: Polaris.txt\n",
            "\n",
            "Extracting Named Entity in the text n°73: The Rats in the Walls.txt\n",
            "\n",
            "Extracting Named Entity in the text n°74: The Hound.txt\n",
            "\n",
            "Extracting Named Entity in the text n°75: .007.txt\n",
            "\n",
            "Extracting Named Entity in the text n°76: The Fall of the House of Usher.txt\n",
            "\n",
            "Extracting Named Entity in the text n°77: Hypnos.txt\n",
            "\n",
            "Extracting Named Entity in the text n°78: The Bet.txt\n",
            "\n",
            "Extracting Named Entity in the text n°79: A Country Doctor.txt\n",
            "\n",
            "Extracting Named Entity in the text n°80: The Picture in the House.txt\n",
            "\n",
            "Extracting Named Entity in the text n°81: The Lagoon.txt\n",
            "\n",
            "Extracting Named Entity in the text n°82: The Voice in the Night.txt\n",
            "\n",
            "Extracting Named Entity in the text n°83: The Descendant.txt\n",
            "\n",
            "Extracting Named Entity in the text n°84: The Evil Clergyman.txt\n",
            "\n",
            "Extracting Named Entity in the text n°85: In the Vault.txt\n",
            "\n",
            "Extracting Named Entity in the text n°86: Ibid.txt\n",
            "\n",
            "Extracting Named Entity in the text n°87: Old Bugs.txt\n",
            "\n",
            "Extracting Named Entity in the text n°88: The Street.txt\n",
            "\n",
            "Extracting Named Entity in the text n°89: Sweet Ermengarde.txt\n",
            "\n",
            "Extracting Named Entity in the text n°90: The Transition of Juan Romero.txt\n",
            "\n",
            "Extracting Named Entity in the text n°91: A Pail of Air.txt\n",
            "\n",
            "Extracting Named Entity in the text n°92: The Legend of Sleepy Hollow.txt\n",
            "\n",
            "Total time elapsed for the task: 2970.452020406723 [s]\n"
          ]
        }
      ],
      "source": [
        "# defined table to save results from the texts\n",
        "output = {}\n",
        "\n",
        "# save the initial time at beginning of the task\n",
        "startTime = time.time()\n",
        "\n",
        "# main loop\n",
        "for idx,path in enumerate(full_list_paths):\n",
        "  try:\n",
        "    file = \" \".join(path.split('/')[3].split('_')[2:])\n",
        "    print(f\"Extracting Named Entity in the text n°{idx+1}: {file}\")\n",
        "  except:\n",
        "    print(f\"Extracting Named Entity in the text n°{idx+1}: {path.split('/')[3].split('_')[2]}\")\n",
        "\n",
        "  sentences = text_from_book(path)\n",
        "  title= path.split('/')[3].replace(\".txt\",\"\")\n",
        "\n",
        "  # pipeline over all the sentences of the text\n",
        "  docs = list(nlp.pipe(sentences))\n",
        "\n",
        "  # initialize counter char\n",
        "  char_counter = 0\n",
        "\n",
        "  # initialize the output vector\n",
        "  y_doc = []\n",
        "\n",
        "  for i,(doc,sentence) in enumerate(zip(docs,sentences)):\n",
        "   \n",
        "    # show for each sentence: tokens, part of speech tagging (POS), dependency parsing and the lemmatization\n",
        "    if verbose:\n",
        "      print(sentence)\n",
        "      print(\"{:<30}\".format(\"-\"*30))\n",
        "      print(\"{:<10}|{:<10}|{:<10}|{:<10}\".format(\"token\",\"DEP\",\"POS\",\"lemma\")) \n",
        "      print(\"{:<30}\".format(\"-\"*30))\n",
        "      for token in doc:\n",
        "        print(f\"{token.text:<10}||{token.dep_:<10}{token.pos_:<10}|{token.lemma_:<10}\")\n",
        "      print(\"\\n\")\n",
        "\n",
        "    # show the results from Named Entity Recognition\n",
        "\n",
        "    tmp_chars = []\n",
        "\n",
        "    plo_ents = get_labels(doc)\n",
        "    if len(plo_ents)> 0:\n",
        "      for ent in plo_ents:\n",
        "        \n",
        "        tmp_row = [title, ent.start_char + char_counter, ent.end_char + char_counter,ent.label_]\n",
        "      \n",
        "        if verbose: print(f\"inserting the following entry: {str(tmp_row)}\")\n",
        "        tmp_chars.append((ent.start_char, ent.end_char)) # local indices for chars \n",
        "        y_doc.append(tmp_row)\n",
        "\n",
        "    char_counter += len(sentence)\n",
        "  \n",
        "  print()\n",
        "\n",
        "  # --- call here functions for optional task and add in each entry ---\n",
        "\n",
        "  # unique name identifying \n",
        "  map_ent_idx, y_doc = set_uniqueNames(y_doc, sentences, \"longer\", False)\n",
        "\n",
        "  # external identification\n",
        "  if EL_RESOURCE == \"babelnet\":\n",
        "    get_babelIDs(sentences, y_doc, map_ent_idx, mode =\"Babelefy\")  # babel ID based on the unique name\n",
        "  else: # retrieve wikidata IDs\n",
        "    get_wikidataID(nlp_el, sentences, y_doc, map_ent_idx, verbose= False)\n",
        "\n",
        "  # store results in the dictionary\n",
        "  output[title] = y_doc\n",
        "\n",
        "print(\"Total time elapsed for the task: {} [s]\".format((time.time() - startTime)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1aRjMCB5CVYx"
      },
      "source": [
        "# Tests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "pp5ac5fo9__U"
      },
      "outputs": [],
      "source": [
        "# toogle testing (to avoid testing when execute all cells)\n",
        "testing = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cSNGkuMFU5wM"
      },
      "source": [
        "## Test  ER module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "xakIOIsw8FpQ"
      },
      "outputs": [],
      "source": [
        "if testing: \n",
        "  # define a small set of tests  \n",
        "  test_1 = \"Josh moved in the USA three weeks ago for business reasons. Now he is working in Google.\"\n",
        "  test_2 = \"Yesterday he visited the White House in Washington D.C\"\n",
        "  test_3 = \"The K2 peak at 8,611 metres (28,251 ft) above sea level, is the second-highest mountain on Earth, after Mount Everest\"\n",
        "  test_4 = \"The Mont Blanc is the highest mountain in the Alps and Western Europe\"\n",
        "  test_5 = \"The lake Lough Neagh is the biggest of the United Kingdom\"\n",
        "  test_6 = \"the Brooklyn bridge is an amazing construction in a borough of New York City\"\n",
        "  test_7 = \"the magic rod picked up in the castle has been broken by Old Richard.\"\n",
        "  full_test = [test_1,test_2,test_3,test_4,test_5,test_6,test_7]\n",
        "\n",
        "  test = full_test\n",
        "\n",
        "  if type(test)==str: # single test\n",
        "    doc = nlp(test)\n",
        "    print(\"number of sentences %d\" % (len(list(doc.sents))))\n",
        "    for ent in doc.ents:\n",
        "      print(ent.text, ent.start_char, ent.end_char, ent.label_)\n",
        "    print(\"----------------\")\n",
        "    plo_ents = get_labels(doc)\n",
        "    for ent in plo_ents:\n",
        "      print(ent.text, ent.start_char, ent.end_char, ent.label_)\n",
        "    print(\"-------------------------------- n\")\n",
        "\n",
        "  else: # the array of tests -> full_test\n",
        "    docs = list(nlp.pipe(test))\n",
        "    print(\"Pipeline type: {}\\n\".format(PIPELINES_TYPE[SIZE_IDX]))\n",
        "    for doc in docs:\n",
        "      for ent in doc.ents:\n",
        "        print(ent.text, ent.start_char, ent.end_char, ent.label_)\n",
        "      print(\"----------------\")\n",
        "      plo_ents = get_labels(doc)\n",
        "      for ent in plo_ents:\n",
        "        print(ent.text, ent.start_char, ent.end_char, ent.label_)\n",
        "      print(\"-------------------------------- n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djEAaTT4Cd3x"
      },
      "source": [
        "## Test unique names "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "RReqHKA3Cl69"
      },
      "outputs": [],
      "source": [
        "if testing:\n",
        "  # define simple dicitonaries for testing \n",
        "  test0_merge = {}\n",
        "  test1_merge = {\"AL\": [[],\"x\"]}\n",
        "  test2_merge = {\n",
        "      \"AL\": [[],\"x\"],\n",
        "      \"B\": [[],\"x\"],\n",
        "      \"A\": [[],\"x\"],\n",
        "      \"L\": [[],\"x\"],\n",
        "      \"X\": [[],\"x\"]\n",
        "  }\n",
        "  test3_merge = {\n",
        "      \"B\": [[],\"x\"],\n",
        "      \"A\": [[],\"x\"],\n",
        "      \"AL\": [[],\"x\"],\n",
        "      \"X\": [[],\"x\"],\n",
        "      \"L\": [[],\"x\"]\n",
        "  }\n",
        "\n",
        "  test4_merge = {\n",
        "      \"ALOE\": [[],\"x\"],\n",
        "      \"A\": [[],\"x\"],\n",
        "      \"AL\": [[],\"x\"],\n",
        "      \"XE\": [[],\"x\"],\n",
        "      \"ALO\": [[],\"x\"],\n",
        "      \"L\": [[],\"x\"],\n",
        "      \"X\": [[],\"x\"],\n",
        "      \"XEN\": [[],\"x\"],\n",
        "  }\n",
        "\n",
        "  test5_merge = {\n",
        "      \"AL\": [[],\"x\"],\n",
        "      \"OE\": [[],\"x\"],\n",
        "      \"A\": [[],\"x\"],\n",
        "      \"O\": [[],\"x\"],\n",
        "      \"L\": [[],\"x\"],\n",
        "      \"E\": [[],\"x\"],\n",
        "      # \"ALOE\": [[],\"x\"],\n",
        "  }\n",
        "\n",
        "  test6_merge = {\n",
        "      \"AL\": [[],\"x\"],\n",
        "      \"OE\": [[],\"x\"],\n",
        "      \"X YY\": [[],\"x\"],\n",
        "      \"W YY\": [[],\"x\"],\n",
        "      \"L\": [[],\"x\"],\n",
        "      \"A\": [[],\"x\"],\n",
        "      \"YY\": [[],\"x\"],\n",
        "      \"ALOE\": [[],\"x\"],\n",
        "  }\n",
        "\n",
        "  test7_merge = {\n",
        "      \"Sir. Franco Grossi\": [[],\"x\"],\n",
        "      \"Franco Grossi\": [[],\"x\"],\n",
        "      \"Ernesto Grossi\": [[],\"x\"],\n",
        "      \"Grossi\": [[],\"x\"],\n",
        "      \"Franco\": [[],\"x\"],\n",
        "  }\n",
        "\n",
        "\n",
        "  def gen_indc_dict(table):\n",
        "    for i,(k,v) in enumerate(table.items()):\n",
        "      for j in range(random.randint(1,4)):\n",
        "        v[0].append(i*4 +j+1)\n",
        "    return table\n",
        "\n",
        "  test = gen_indc_dict(test7_merge)\n",
        "\n",
        "\n",
        "  # --- launch test\n",
        "  criterion = sorting_criterions[\"first\"]\n",
        "  test = dict(sorted(test.items(), key = criterion))\n",
        "\n",
        "  print_table(test)\n",
        "\n",
        "  if len(test) > 1: edited = merge_items(test, debug = True)\n",
        "  print(test)\n",
        "  while(edited and len(test) > 1):\n",
        "    edited =  merge_items(test, debug = True)\n",
        "    print(test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2gTvtIcDu1c"
      },
      "source": [
        "# Generate .tsv file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "UVwnbvIxEiUa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "32a5b636-9660-4328-f634-ceeedfdcd0d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "elaborating results from bn_00173084n_The Old Dame and her Hen...\n",
            "elaborating results from bn_00187346n_The Dove...\n",
            "elaborating results from bn_00187442n_Corvetto...\n",
            "elaborating results from bn_00608721n_Sun, Moon, and Talia...\n",
            "elaborating results from bn_00790303n_Childe Rowland...\n",
            "elaborating results from bn_01593735n_Town Musicians of Bremen...\n",
            "elaborating results from bn_01899260n_The Raven...\n",
            "elaborating results from bn_02154461n_Herr Korbes...\n",
            "elaborating results from bn_02173122n_Thumbelina...\n",
            "elaborating results from bn_02238889n_The Nightingale...\n",
            "elaborating results from bn_02277659n_The Ugly Duckling...\n",
            "elaborating results from bn_02442758n_Jack the Giant Killer...\n",
            "elaborating results from bn_03300215n_The Wonderful Birch...\n",
            "elaborating results from bn_03301753n_Cap-o_-Rushes...\n",
            "elaborating results from bn_03326399n_The Story of Pretty Goldilocks...\n",
            "elaborating results from bn_03329137n_Soria Moria Castle...\n",
            "elaborating results from bn_03329494n_The Cat on the Dovrefjell...\n",
            "elaborating results from bn_03331799n_The Language of the Birds...\n",
            "elaborating results from bn_03332887n_Petrosinella...\n",
            "elaborating results from bn_03364076n_Princess Rosette...\n",
            "elaborating results from bn_03377704n_Gold-Tree and Silver-Tree...\n",
            "elaborating results from bn_03393628n_Cannetella...\n",
            "elaborating results from bn_03505587n_The Ram...\n",
            "elaborating results from bn_03526381n_Ferdinand the Faithful and Ferdinand the Unfaithful...\n",
            "elaborating results from bn_03550932n_The She-bear...\n",
            "elaborating results from bn_03661112n_The Tale of Tsar Saltan...\n",
            "elaborating results from bn_03729159n_Pintosmalto...\n",
            "elaborating results from bn_03729229n_The Love for Three Oranges...\n",
            "elaborating results from bn_03730572n_The Months...\n",
            "elaborating results from bn_03813035n_Snow-White and Rose-Red...\n",
            "elaborating results from bn_03884049n_The Enchanted Snake...\n",
            "elaborating results from bn_03980496n_The Jew Among Thorns...\n",
            "elaborating results from bn_14140242n_The Frog Prince...\n",
            "elaborating results from bn_14145915n_The Boy Who Had an Eating Match with a Troll...\n",
            "elaborating results from bn_15417261n_Jack and the Beanstalk...\n",
            "elaborating results from bn_17385267n_The Brave Little Tailor...\n",
            "elaborating results from bn_21706209n_Bluebeard...\n",
            "elaborating results from bn_21706329n_Cupid and Psyche...\n",
            "elaborating results from bn_00183025n_Rab and his Friends...\n",
            "elaborating results from bn_00210737n_Alyosha the Pot...\n",
            "elaborating results from bn_00283865n_The Great Carbuncle...\n",
            "elaborating results from bn_00288807n_The Man of Adamant...\n",
            "elaborating results from bn_00645621n_A Descent into the Maelström...\n",
            "elaborating results from bn_00655138n_Thubway Tham_s Inthane Moment...\n",
            "elaborating results from bn_00663751n_The Facts in the Case of M. Valdemar...\n",
            "elaborating results from bn_00955099n_The Nameless City...\n",
            "elaborating results from bn_01190519n_The Emperor_s New Clothes...\n",
            "elaborating results from bn_01304399n_The Music of Erich Zann...\n",
            "elaborating results from bn_01304444n_The Cats of Ulthar...\n",
            "elaborating results from bn_01323408n_The Damned Thing...\n",
            "elaborating results from bn_01425478n_The Vengeance of Nitocris...\n",
            "elaborating results from bn_01436803n_The Answer...\n",
            "elaborating results from bn_01521190n_Claude Gueux...\n",
            "elaborating results from bn_01630488n_The Haunter of the Dark...\n",
            "elaborating results from bn_01644148n_Pickman_s Model...\n",
            "elaborating results from bn_01745953n_An Occurrence at Owl Creek Bridge...\n",
            "elaborating results from bn_01789382n_Rip Van Winkle...\n",
            "elaborating results from bn_02107348n_The Isle of Voices...\n",
            "elaborating results from bn_02201468n_The Doom That Came to Sarnath...\n",
            "elaborating results from bn_02275371n_The Outsider...\n",
            "elaborating results from bn_02413967n_The Black Cat...\n",
            "elaborating results from bn_02439324n_Micromégas...\n",
            "elaborating results from bn_02464343n_From Beyond...\n",
            "elaborating results from bn_02485046n_Brooksmith...\n",
            "elaborating results from bn_02502064n_Facino Cane...\n",
            "elaborating results from bn_02767919n_To Whom This May Come...\n",
            "elaborating results from bn_02975525n_Rothschild_s Violin...\n",
            "elaborating results from bn_03079213n_The Quest of Iranon...\n",
            "elaborating results from bn_03149445n_The Tree...\n",
            "elaborating results from bn_03149511n_The Tomb...\n",
            "elaborating results from bn_03176704n_The Gift of the Magi...\n",
            "elaborating results from bn_03214242n_Polaris...\n",
            "elaborating results from bn_03220342n_The Rats in the Walls...\n",
            "elaborating results from bn_03280831n_The Hound...\n",
            "elaborating results from bn_03297228n_.007...\n",
            "elaborating results from bn_03341625n_The Fall of the House of Usher...\n",
            "elaborating results from bn_03396386n_Hypnos...\n",
            "elaborating results from bn_03396420n_The Bet...\n",
            "elaborating results from bn_03419493n_A Country Doctor...\n",
            "elaborating results from bn_03441968n_The Picture in the House...\n",
            "elaborating results from bn_03453004n_The Lagoon...\n",
            "elaborating results from bn_03461916n_The Voice in the Night...\n",
            "elaborating results from bn_03518160n_The Descendant...\n",
            "elaborating results from bn_03520887n_The Evil Clergyman...\n",
            "elaborating results from bn_03520895n_In the Vault...\n",
            "elaborating results from bn_03520903n_Ibid...\n",
            "elaborating results from bn_03520905n_Old Bugs...\n",
            "elaborating results from bn_03520913n_The Street...\n",
            "elaborating results from bn_03520916n_Sweet Ermengarde...\n",
            "elaborating results from bn_03520917n_The Transition of Juan Romero...\n",
            "elaborating results from bn_03655527n_A Pail of Air...\n",
            "elaborating results from bn_03729761n_The Legend of Sleepy Hollow...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_967259cb-4626-4145-8522-4edec7ff1a74\", \"result.tsv\", 665358)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "\"\"\"\n",
        "Directives:\n",
        "- not include header in the tsv file\n",
        "- fields: \"text_id\",\"start_offset\",\"end_offset\",\"label\",\"name\",\"resource_id\"\n",
        "- \"name\" and \"resource_id\" labels are optional\n",
        "\"\"\"\n",
        "\n",
        "# generate the full list of the final result \n",
        "full_list_rows = []\n",
        "for title,y_doc in output.items():\n",
        "  print(f\"elaborating results from {title}...\")\n",
        "  full_list_rows.extend(y_doc)\n",
        "\n",
        "# build the dataframe\n",
        "df = pd.DataFrame(full_list_rows)\n",
        "\n",
        "# save the dataframe as .tsv file\n",
        "df.to_csv(path_or_buf= \"result.tsv\",sep=\"\\t\",header=False, index = False, encoding='utf-16')\n",
        "# download and save locally the result file \n",
        "files.download('result.tsv')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "toc_visible": true,
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}